{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imports\n",
    "import os\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from loguru import logger\n",
    "# local imports\n",
    "from ingest.ingester import Ingester\n",
    "from query.querier import Querier\n",
    "from summarize.summarizer import Summarizer\n",
    "import settings\n",
    "import utils as ut\n",
    "from query.querier import EnumMode\n",
    "from ingest.ingester import IngestionMode\n",
    "from datetime import datetime\n",
    "import kamervragenEvaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT= \"\"\"\n",
    "### OBJECTIVE ###\n",
    "Je bent een assistent voor de rijksoverheid. Jouw taak is om vragen te beantwoorden in het Nederlands. Zorg ervoor dat je alleen antwoord geeft op basis van de beschikbare context en dat je daar ook naar verwijst in je antwoord.\n",
    "\n",
    "### AUDIENCE ###\n",
    "De doelgroep van jouw antwoorden zijn ambtenaren. Geef alle relevante informatie uit de context, antwoord in het Nederlands leg in maximaal 100 woorden zoveel mogelijk uit.\n",
    "\n",
    "### GUARDRAILS ###\n",
    "Indien de context onvoldoende informatie bevat om de vraag te beantwoorden, verzin dan geen informatie maar geef aan dat er onvoldoende informatie beschikbaar is.\n",
    "\n",
    "### INSTRUCTIONS ###\n",
    "- Beantwoord de vraag altijd in het Nederlands, zelfs als de context in het Engels is gesteld.\n",
    "- Vermijd het herhalen van de vraag in het antwoord en het herhalen van de instructies. Voer de instructies uit en geef een concreet antwoord op de gestelde vraag.\n",
    "- Geef een stapsgewijze redenering bij het beantwoorden van de vraag en refereer naar specifieke zinnen uit de context die hebben bijgedragen aan het antwoord.\n",
    "- Houd je antwoord nauw verbonden met de context en vermijd het toevoegen van informatie die niet expliciet in de context wordt vermeld.\n",
    "\n",
    "- Voor meer informatie over de context, zeg het bestandsnaam die gevonden is in de source_document. Mits deze beschikbaar is.\n",
    "### QUESTION ### \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Commented the settings that arent used as parameters in the functions\n",
    "\n",
    "# DOC_DIR = \"./docs\"\n",
    "# CHUNK_DIR = \"./chunks\"\n",
    "# VECDB_DIR = \"./vector_stores\"\n",
    "# EVAL_DIR = \"./evaluate\"\n",
    "# EVAL_APP_HEADER = \"Evaluation\"\n",
    "# EVAL_APP_INFO = \"./info/evaluation_explanation.txt\"\n",
    "# EVAL_FILE_NAME = \"eval.json\"\n",
    "# CHAIN_VERBOSITY = False\n",
    "LLM_TYPE = \"local_llm\"\n",
    "LLM_MODEL_TYPE = \"gemma2\"\n",
    "# API_URL = \"http://127.0.0.1:11434\"\n",
    "AZUREOPENAI_API_VERSION = \"2023-08-01-preview\"\n",
    "EMBEDDINGS_PROVIDER = \"local_embeddings\"\n",
    "EMBEDDINGS_MODEL = \"textgain/allnli-GroNLP-bert-base-dutch-cased\"\n",
    "TEXT_SPLITTER_METHOD = \"NLTKTextSplitter\"\n",
    "# CHAIN_NAME = \"conversationalretrievalchain\"\n",
    "# CHAIN_TYPE = \"stuff\"\n",
    "# SEARCH_TYPE = \"similarity\"\n",
    "# SCORE_THRESHOLD = 0.5\n",
    "VECDB_TYPE = \"chromadb\"\n",
    "CHUNK_SIZE = 1024\n",
    "# CHUNK_K = 4\n",
    "CHUNK_OVERLAP = 256\n",
    "# RETRIEVAL_METHOD = \"regular\"\n",
    "\n",
    "\n",
    "folderSelected = \"kamerVragen\"\n",
    "my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected)\n",
    "\n",
    "CONCAT_FILES = True\n",
    "question_sample_CSV = \"question_sample.csv\"\n",
    "VALIDATIONLAPS = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTING_METHODS = [IngestionMode.question_answer,IngestionMode.token_small,IngestionMode.token_medium,IngestionMode.token_large]\n",
    "CONTEXT_PRESENT= [True, False]\n",
    "EMBEDDINGS_MODELS = [\"GroNLP/bert-base-dutch-cased\",\"textgain/allnli-GroNLP-bert-base-dutch-cased\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\"dunzhang/stella_en_400M_v5\", \"actualdata/jina-embeddings-v3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "querier = None\n",
    "ingester = None\n",
    "\n",
    "def init(LLM_TYPE=LLM_TYPE, LLM_MODEL_TYPE=LLM_MODEL_TYPE, EMBEDDINGS_MODEL=EMBEDDINGS_MODEL, EMBEDDINGS_PROVIDER=EMBEDDINGS_PROVIDER, AZUREOPENAI_API_VERSION=AZUREOPENAI_API_VERSION, TEXT_SPLITTER_METHOD=TEXT_SPLITTER_METHOD, CHUNK_SIZE=CHUNK_SIZE, CHUNK_OVERLAP=CHUNK_OVERLAP, VECDB_TYPE=VECDB_TYPE, vectordb_folder= my_vectordb_folder_path_selected, content_folder=my_folder_path_selected):\n",
    "  # Init\n",
    "  querier = Querier(\n",
    "    llm_type=LLM_TYPE, \n",
    "    llm_model_type=LLM_MODEL_TYPE, \n",
    "    embeddings_model=EMBEDDINGS_MODEL, \n",
    "    embeddings_provider=EMBEDDINGS_PROVIDER, \n",
    "    azureopenai_api_version=AZUREOPENAI_API_VERSION\n",
    "    )\n",
    "\n",
    "  ingester = Ingester(\n",
    "    collection_name=folderSelected, \n",
    "    content_folder=content_folder, \n",
    "    vectordb_folder=vectordb_folder,\n",
    "    embeddings_model=EMBEDDINGS_MODEL,\n",
    "    text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    vecdb_type=VECDB_TYPE\n",
    "    )\n",
    "  return [querier,ingester]\n",
    "  \n",
    "# querier,ingester = init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(mode=IngestionMode.question_answer_per_page, forceRebuild=True, addedMetaDataURLCSV=\"docs/metadata.csv\", addContext=True):\n",
    "  ingester.ingest(mode=mode, forceRebuild=forceRebuild, addedMetaDataURLCSV=addedMetaDataURLCSV, addContext=addContext)\n",
    "# ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(vectorDBPATH = my_vectordb_folder_path_selected):\n",
    "  querier.make_chain(folderSelected, vectorDBPATH)\n",
    "# chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_error_log(error, params):\n",
    "  with open(\"error_log.txt\", \"a\") as f:\n",
    "    f.write(f\"{datetime.now()} | {params} - {error}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markheijnekamp/Documents/GitHub/LearningLion-kamervragen/utils.py:113: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-10-03 12:24:54.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: textgain/allnli-GroNLP-bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.257\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.942\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:54.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.264\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-03 12:24:55.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done writing to csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "querier,ingester = init(EMBEDDINGS_MODEL=EMBEDDINGS_MODEL, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "kamervragenEvaluation.create_evaluation_sample_questions(my_folder_path_selected,ingester=ingester, destinationCSV=question_sample_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place folder contents in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "      for i in range(VALIDATIONLAPS):\n",
    "        current_item += 1\n",
    "        try:\n",
    "          chunk_size = CHUNK_SIZE\n",
    "          if splittingMethod == IngestionMode.token_small:\n",
    "            chunk_size = 128\n",
    "          elif splittingMethod == IngestionMode.token_medium:\n",
    "            chunk_size = 512\n",
    "          elif splittingMethod == IngestionMode.token_large:\n",
    "            chunk_size = 1024\n",
    "            \n",
    "          # Setup\n",
    "          my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "          querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "          # Ingestion\n",
    "          ingest(mode=splittingMethod, addContext=context, addedMetaDataURLCSV=\"docs/metadata.csv\")\n",
    "          # RAG CHAIN\n",
    "          chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "          \n",
    "          # Evaluation\n",
    "          kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "            question_sample_CSV,querier=querier, \n",
    "            toCSV=True, \n",
    "            ingestionMode=splittingMethod, \n",
    "            addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "            addContext=context,\n",
    "            embeddings_model=embeddingModel,\n",
    "            text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "            embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "            database=VECDB_TYPE,\n",
    "            concatFiles=CONCAT_FILES,\n",
    "            )\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          # Write error to file\n",
    "          write_to_error_log(e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "          continue\n",
    "        \n",
    "        \n",
    "        print(f\"done with {current_item} of {total_items}\")\n",
    "  print(f\"Done with iteration {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LongTimeEmbeddingsModels = [\"BAAI/bge-multilingual-gemma2\", \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\"Alibaba-NLP/gte-multilingual-base\"]\n",
    "\n",
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "      for i in range(VALIDATIONLAPS):\n",
    "        current_item += 1\n",
    "        try:\n",
    "          chunk_size = CHUNK_SIZE\n",
    "          if splittingMethod == IngestionMode.token_small:\n",
    "            chunk_size = 128\n",
    "          elif splittingMethod == IngestionMode.token_medium:\n",
    "            chunk_size = 512\n",
    "          elif splittingMethod == IngestionMode.token_large:\n",
    "            chunk_size = 1024\n",
    "            \n",
    "          # Setup\n",
    "          my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "          querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "          # Ingestion\n",
    "          ingest(mode=splittingMethod, addContext=context, addedMetaDataURLCSV=\"docs/metadata.csv\")\n",
    "          # RAG CHAIN\n",
    "          chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "          \n",
    "          # Evaluation\n",
    "          kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "            question_sample_CSV,querier=querier, \n",
    "            toCSV=True, \n",
    "            ingestionMode=splittingMethod, \n",
    "            addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "            addContext=context,\n",
    "            embeddings_model=embeddingModel,\n",
    "            text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "            embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "            database=VECDB_TYPE,\n",
    "            concatFiles=CONCAT_FILES,\n",
    "            )\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          # Write error to file\n",
    "          write_to_error_log(e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "          continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"done with {current_item} of {total_items}\")\n",
    "  print(f\"Done with iteration {time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re test items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add context to question_sample.csv\n",
    "import pandas as pd\n",
    "\n",
    "from ingest.file_parser import FileParser\n",
    "from ingest.ingest_utils import IngestUtils\n",
    "\n",
    "df = pd.read_csv(question_sample_CSV)\n",
    "file_parser = FileParser()\n",
    "ingestutils = IngestUtils(CHUNK_SIZE, 0, None, TEXT_SPLITTER_METHOD)\n",
    "if 'context' not in df.columns:\n",
    "  for index, row in df.iterrows():\n",
    "    # get file\n",
    "    file = row[\"Filename\"]\n",
    "    if file.endswith(\".pdf\"):\n",
    "      file_path = os.path.join(my_folder_path_selected, file)\n",
    "      raw_pages, metadata = file_parser.parse_file(file_path)\n",
    "      documents = ingestutils.clean_text_to_docs(raw_pages, metadata)\n",
    "      introduction = documents[0].page_content.split(\"vraag\")[0]\n",
    "\n",
    "    df.at[index, \"context\"] = introduction\n",
    "    df.to_csv(question_sample_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "        for i in range(VALIDATIONLAPS):\n",
    "          current_item += 1\n",
    "          try:\n",
    "            chunk_size = CHUNK_SIZE\n",
    "            if splittingMethod == IngestionMode.token_small:\n",
    "              chunk_size = 128\n",
    "            elif splittingMethod == IngestionMode.token_medium:\n",
    "              chunk_size = 512\n",
    "            elif splittingMethod == IngestionMode.token_large:\n",
    "              chunk_size = 1024\n",
    "              \n",
    "            # Setup\n",
    "            my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "            querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "            # RAG CHAIN\n",
    "            chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "            \n",
    "            # Evaluation\n",
    "            kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "              question_sample_CSV,querier=querier, \n",
    "              toCSV=True, \n",
    "              ingestionMode=splittingMethod, \n",
    "              addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "              addContext=context,\n",
    "              embeddings_model=embeddingModel,\n",
    "              text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "              embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "              database=VECDB_TYPE,\n",
    "              concatFiles=CONCAT_FILES,\n",
    "              )\n",
    "          except Exception as e:\n",
    "            print(e)\n",
    "            # Write error to file\n",
    "            write_to_error_log(e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "            continue\n",
    "          \n",
    "          \n",
    "          \n",
    "          print(f\"done with {current_item} of {total_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kamervragenEvaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if one single file can be retrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kamervragenEvaluation.test_retrival(\n",
    "  my_folder_path_selected, \n",
    "  ingester, \n",
    "  querier=querier, \n",
    "  toCSV=True,\n",
    "  ingestionMode=IngestionMode.question_answer_per_page, \n",
    "  addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "  addContext=True,\n",
    "  embeddings_model=EMBEDDINGS_MODEL,\n",
    "  text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "  embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "  database=VECDB_TYPE,\n",
    "  ConcatFiles=CONCAT_FILES\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamervragenEvaluation.store_questions_and_answers_CSV(my_folder_path_selected, ingester,concatFiles=CONCAT_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamervragenEvaluation.test_retrival_map_grading(\n",
    "  my_folder_path_selected, \n",
    "  ingester, \n",
    "  querier=querier, \n",
    "  toCSV=True,\n",
    "  ingestionMode=IngestionMode.question_answer_per_page, \n",
    "  addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "  addContext=True,\n",
    "  embeddings_model=EMBEDDINGS_MODEL,\n",
    "  text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "  embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "  database=VECDB_TYPE,\n",
    "  concatFiles=CONCAT_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
