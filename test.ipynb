{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imports\n",
    "import os\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from loguru import logger\n",
    "# local imports\n",
    "from ingest.ingester import Ingester\n",
    "from query.querier import Querier\n",
    "from summarize.summarizer import Summarizer\n",
    "import settings\n",
    "import utils as ut\n",
    "from query.querier import EnumMode\n",
    "from ingest.ingester import IngestionMode\n",
    "from datetime import datetime\n",
    "import kamervragenEvaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT= \"\"\"\n",
    "### OBJECTIVE ###\n",
    "Je bent een assistent voor de rijksoverheid. Jouw taak is om vragen te beantwoorden in het Nederlands. Zorg ervoor dat je alleen antwoord geeft op basis van de beschikbare context en dat je daar ook naar verwijst in je antwoord.\n",
    "\n",
    "### AUDIENCE ###\n",
    "De doelgroep van jouw antwoorden zijn ambtenaren. Geef alle relevante informatie uit de context, antwoord in het Nederlands leg in maximaal 100 woorden zoveel mogelijk uit.\n",
    "\n",
    "### GUARDRAILS ###\n",
    "Indien de context onvoldoende informatie bevat om de vraag te beantwoorden, verzin dan geen informatie maar geef aan dat er onvoldoende informatie beschikbaar is.\n",
    "\n",
    "### INSTRUCTIONS ###\n",
    "- Beantwoord de vraag altijd in het Nederlands, zelfs als de context in het Engels is gesteld.\n",
    "- Vermijd het herhalen van de vraag in het antwoord en het herhalen van de instructies. Voer de instructies uit en geef een concreet antwoord op de gestelde vraag.\n",
    "- Geef een stapsgewijze redenering bij het beantwoorden van de vraag en refereer naar specifieke zinnen uit de context die hebben bijgedragen aan het antwoord.\n",
    "- Houd je antwoord nauw verbonden met de context en vermijd het toevoegen van informatie die niet expliciet in de context wordt vermeld.\n",
    "\n",
    "- Voor meer informatie over de context, zeg het bestandsnaam die gevonden is in de source_document. Mits deze beschikbaar is.\n",
    "### QUESTION ### \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Commented the settings that arent used as parameters in the functions\n",
    "\n",
    "# DOC_DIR = \"./docs\"\n",
    "# CHUNK_DIR = \"./chunks\"\n",
    "# VECDB_DIR = \"./vector_stores\"\n",
    "# EVAL_DIR = \"./evaluate\"\n",
    "# EVAL_APP_HEADER = \"Evaluation\"\n",
    "# EVAL_APP_INFO = \"./info/evaluation_explanation.txt\"\n",
    "# EVAL_FILE_NAME = \"eval.json\"\n",
    "# CHAIN_VERBOSITY = False\n",
    "LLM_TYPE = \"local_llm\"\n",
    "LLM_MODEL_TYPE = \"gemma2\"\n",
    "# API_URL = \"http://127.0.0.1:11434\"\n",
    "AZUREOPENAI_API_VERSION = \"2023-08-01-preview\"\n",
    "EMBEDDINGS_PROVIDER = \"local_embeddings\"\n",
    "EMBEDDINGS_MODEL = \"textgain/allnli-GroNLP-bert-base-dutch-cased\"\n",
    "TEXT_SPLITTER_METHOD = \"NLTKTextSplitter\"\n",
    "# CHAIN_NAME = \"conversationalretrievalchain\"\n",
    "# CHAIN_TYPE = \"stuff\"\n",
    "# SEARCH_TYPE = \"similarity\"\n",
    "# SCORE_THRESHOLD = 0.5\n",
    "VECDB_TYPE = \"chromadb\"\n",
    "CHUNK_SIZE = 1024\n",
    "# CHUNK_K = 4\n",
    "CHUNK_OVERLAP = 256\n",
    "# RETRIEVAL_METHOD = \"regular\"\n",
    "\n",
    "\n",
    "folderSelected = \"kamerVragen\"\n",
    "my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected)\n",
    "\n",
    "CONCAT_FILES = True\n",
    "question_sample_CSV = \"question_sample.csv\"\n",
    "VALIDATIONLAPS = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTING_METHODS = [IngestionMode.question_answer,IngestionMode.token_small,IngestionMode.token_medium,IngestionMode.token_large]\n",
    "CONTEXT_PRESENT= [True, False]\n",
    "EMBEDDINGS_MODELS = [\"GroNLP/bert-base-dutch-cased\",\"textgain/allnli-GroNLP-bert-base-dutch-cased\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\"dunzhang/stella_en_400M_v5\", \"actualdata/jina-embeddings-v3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "querier = None\n",
    "ingester = None\n",
    "\n",
    "def init(LLM_TYPE=LLM_TYPE, LLM_MODEL_TYPE=LLM_MODEL_TYPE, EMBEDDINGS_MODEL=EMBEDDINGS_MODEL, EMBEDDINGS_PROVIDER=EMBEDDINGS_PROVIDER, AZUREOPENAI_API_VERSION=AZUREOPENAI_API_VERSION, TEXT_SPLITTER_METHOD=TEXT_SPLITTER_METHOD, CHUNK_SIZE=CHUNK_SIZE, CHUNK_OVERLAP=CHUNK_OVERLAP, VECDB_TYPE=VECDB_TYPE, vectordb_folder= my_vectordb_folder_path_selected, content_folder=my_folder_path_selected):\n",
    "  # Init\n",
    "  querier = Querier(\n",
    "    llm_type=LLM_TYPE, \n",
    "    llm_model_type=LLM_MODEL_TYPE, \n",
    "    embeddings_model=EMBEDDINGS_MODEL, \n",
    "    embeddings_provider=EMBEDDINGS_PROVIDER, \n",
    "    azureopenai_api_version=AZUREOPENAI_API_VERSION\n",
    "    )\n",
    "\n",
    "  ingester = Ingester(\n",
    "    collection_name=folderSelected, \n",
    "    content_folder=content_folder, \n",
    "    vectordb_folder=vectordb_folder,\n",
    "    embeddings_model=EMBEDDINGS_MODEL,\n",
    "    text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    vecdb_type=VECDB_TYPE\n",
    "    )\n",
    "  return [querier,ingester]\n",
    "  \n",
    "# querier,ingester = init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(mode=IngestionMode.question_answer_per_page, forceRebuild=True, addedMetaDataURLCSV=\"docs/metadata.csv\", addContext=True):\n",
    "  \"\"\"Ingest the documents in the folder\"\"\"\n",
    "  ingester.ingest(mode=mode, forceRebuild=forceRebuild, addedMetaDataURLCSV=addedMetaDataURLCSV, addContext=addContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(vectorDBPATH = my_vectordb_folder_path_selected):\n",
    "  \"\"\"Build the RAG Chain\"\"\"\n",
    "  querier.make_chain(folderSelected, vectorDBPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample questions\n",
    "\n",
    "If no sample is provided the tests will not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "querier,ingester = init(EMBEDDINGS_MODEL=EMBEDDINGS_MODEL, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "kamervragenEvaluation.create_evaluation_sample_questions(my_folder_path_selected,ingester=ingester, destinationCSV=question_sample_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ingest and evaluate\n",
    "\n",
    "Ingest the questions and evaluate the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from error import write_to_error_log\n",
    "\n",
    "\n",
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "      for i in range(VALIDATIONLAPS):\n",
    "        current_item += 1\n",
    "        try:\n",
    "          chunk_size = CHUNK_SIZE\n",
    "          if splittingMethod == IngestionMode.token_small:\n",
    "            chunk_size = 128\n",
    "          elif splittingMethod == IngestionMode.token_medium:\n",
    "            chunk_size = 512\n",
    "          elif splittingMethod == IngestionMode.token_large:\n",
    "            chunk_size = 1024\n",
    "            \n",
    "          # Setup\n",
    "          my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "          querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "          # Ingestion\n",
    "          ingest(mode=splittingMethod, addContext=context, addedMetaDataURLCSV=\"docs/metadata.csv\")\n",
    "          # RAG CHAIN\n",
    "          chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "          \n",
    "          # Evaluation\n",
    "          kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "            question_sample_CSV,querier=querier, \n",
    "            toCSV=True, \n",
    "            ingestionMode=splittingMethod, \n",
    "            addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "            addContext=context,\n",
    "            embeddings_model=embeddingModel,\n",
    "            text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "            embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "            database=VECDB_TYPE,\n",
    "            concatFiles=CONCAT_FILES,\n",
    "            )\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          # Write error to file\n",
    "          write_to_error_log(\"error_log_test.txt\", e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "          continue\n",
    "        \n",
    "        \n",
    "        print(f\"done with {current_item} of {total_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and evaluate larger models on the same data\n",
    "The larger models selected are  \"BAAI/bge-multilingual-gemma2\", \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\"Alibaba-NLP/gte-multilingual-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LongTimeEmbeddingsModels = [\"BAAI/bge-multilingual-gemma2\", \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\"Alibaba-NLP/gte-multilingual-base\"]\n",
    "\n",
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "      for i in range(VALIDATIONLAPS):\n",
    "        current_item += 1\n",
    "        try:\n",
    "          chunk_size = CHUNK_SIZE\n",
    "          if splittingMethod == IngestionMode.token_small:\n",
    "            chunk_size = 128\n",
    "          elif splittingMethod == IngestionMode.token_medium:\n",
    "            chunk_size = 512\n",
    "          elif splittingMethod == IngestionMode.token_large:\n",
    "            chunk_size = 1024\n",
    "            \n",
    "          # Setup\n",
    "          my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "          querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "          # Ingestion\n",
    "          ingest(mode=splittingMethod, addContext=context, addedMetaDataURLCSV=\"docs/metadata.csv\")\n",
    "          # RAG CHAIN\n",
    "          chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "          \n",
    "          # Evaluation\n",
    "          kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "            question_sample_CSV,querier=querier, \n",
    "            toCSV=True, \n",
    "            ingestionMode=splittingMethod, \n",
    "            addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "            addContext=context,\n",
    "            embeddings_model=embeddingModel,\n",
    "            text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "            embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "            database=VECDB_TYPE,\n",
    "            concatFiles=CONCAT_FILES,\n",
    "            )\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          # Write error to file\n",
    "          write_to_error_log(e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "          continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"done with {current_item} of {total_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-test items\n",
    "\n",
    "This uses the existing database that has already been ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "        for i in range(VALIDATIONLAPS):\n",
    "          current_item += 1\n",
    "          try:\n",
    "            chunk_size = CHUNK_SIZE\n",
    "            if splittingMethod == IngestionMode.token_small:\n",
    "              chunk_size = 128\n",
    "            elif splittingMethod == IngestionMode.token_medium:\n",
    "              chunk_size = 512\n",
    "            elif splittingMethod == IngestionMode.token_large:\n",
    "              chunk_size = 1024\n",
    "              \n",
    "            # Setup\n",
    "            my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "            querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "            # RAG CHAIN\n",
    "            chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "            \n",
    "            # Evaluation\n",
    "            kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "              question_sample_CSV,querier=querier, \n",
    "              toCSV=True, \n",
    "              ingestionMode=splittingMethod, \n",
    "              addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "              addContext=context,\n",
    "              embeddings_model=embeddingModel,\n",
    "              text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "              embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "              database=VECDB_TYPE,\n",
    "              concatFiles=CONCAT_FILES,\n",
    "              )\n",
    "          except Exception as e:\n",
    "            print(e)\n",
    "            # Write error to file\n",
    "            write_to_error_log(e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "            continue\n",
    "          \n",
    "          \n",
    "          \n",
    "          print(f\"done with {current_item} of {total_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for embeddingModel in EMBEDDINGS_MODELS:\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "        for i in range(VALIDATIONLAPS):\n",
    "          current_item += 1\n",
    "          try:\n",
    "            chunk_size = CHUNK_SIZE\n",
    "            if splittingMethod == IngestionMode.token_small:\n",
    "              chunk_size = 128\n",
    "            elif splittingMethod == IngestionMode.token_medium:\n",
    "              chunk_size = 512\n",
    "            elif splittingMethod == IngestionMode.token_large:\n",
    "              chunk_size = 1024\n",
    "              \n",
    "            # Setup\n",
    "            my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "            querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "            # RAG CHAIN\n",
    "            chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "            \n",
    "            # Evaluation\n",
    "            kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "              question_sample_CSV,querier=querier, \n",
    "              toCSV=True, \n",
    "              ingestionMode=splittingMethod, \n",
    "              addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "              addContext=context,\n",
    "              embeddings_model=embeddingModel,\n",
    "              text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "              embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "              database=VECDB_TYPE,\n",
    "              concatFiles=CONCAT_FILES,\n",
    "              )\n",
    "          except Exception as e:\n",
    "            print(e)\n",
    "            # Write error to file\n",
    "            write_to_error_log(e, f\"splittingMethod={splittingMethod}, context={context}, embeddingModel={embeddingModel}\")\n",
    "            continue\n",
    "          \n",
    "          \n",
    "          \n",
    "          print(f\"done with {current_item} of {total_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "splittingMethod = IngestionMode.token_small\n",
    "embeddingModel = \"textgain/allnli-GroNLP-bert-base-dutch-cased\"\n",
    "context = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markheijnekamp/Documents/GitHub/LearningLion-kamervragen/utils.py:121: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-10-03 16:31:17.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mLoaded local embeddings: textgain/allnli-GroNLP-bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-03 16:31:17.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-03 16:31:17.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-03 16:31:17.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-03 16:31:17.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "/Users/markheijnekamp/Documents/GitHub/LearningLion-kamervragen/utils.py:61: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n",
      "\u001b[32m2024-10-03 16:31:17.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mquery.querier\u001b[0m:\u001b[36mmake_chain\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mLoaded chromadb from folder ./vector_stores/kamerVragenchromadb_128_0_local_embeddings_textgain-allnli-GroNLP-bert-base-dutch-cased_False_None_VECDB_TYPE_IngestionMode-token_small\u001b[0m\n",
      "\u001b[32m2024-10-03 16:31:17.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mquery.querier\u001b[0m:\u001b[36mmake_chain\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mExecuted Querier.make_chain\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "chain(vectorDBPATH=my_vectordb_folder_path_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-03 16:32:07.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mquery.querier\u001b[0m:\u001b[36mget_documents_with_scores\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mTopscore most similar docs: 0.9825565814971924\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'Source': 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/Document(0aad043d-7bba-42fe-acb1-16b78ffdc59c)/resource', 'Subject': \"Antwoord op vragen van de leden Tjeerd de Groot en Sneller over het bericht 'Werkgevers: bedrijven verplaatsen nieuwe investeringen naar het buitenland'\", 'author': '', 'chunk': 5, 'filename': '0aad043d-7bba-42fe-acb1-16b78ffdc59c.pdf', 'page_number': 1, 'source': 'p1-5', 'title': 'None'}, page_content='Vraag 2 Kunt u in kaart brengen hoeveel en mogelijk welke bedrijven nieuwe investeringen doen in het buitenland in plaats van Nederland, of dit overwegen?'),\n",
       "  0.9825565814971924),\n",
       " (Document(metadata={'Source': 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/Document(0aad043d-7bba-42fe-acb1-16b78ffdc59c)/resource', 'Subject': \"Antwoord op vragen van de leden Tjeerd de Groot en Sneller over het bericht 'Werkgevers: bedrijven verplaatsen nieuwe investeringen naar het buitenland'\", 'author': '', 'chunk': 7, 'filename': '0aad043d-7bba-42fe-acb1-16b78ffdc59c.pdf', 'page_number': 1, 'source': 'p1-7', 'title': 'None'}, page_content='Antwoord 2 Hoeveel en mogelijk welke bedrijven nieuwe investeringen doen in het buitenland in plaats van Nederland, of dit overwegen is lastig in exacte aantallen te vatten.'),\n",
       "  0.8837676048278809),\n",
       " (Document(metadata={'Source': 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/Document(0aad043d-7bba-42fe-acb1-16b78ffdc59c)/resource', 'Subject': \"Antwoord op vragen van de leden Tjeerd de Groot en Sneller over het bericht 'Werkgevers: bedrijven verplaatsen nieuwe investeringen naar het buitenland'\", 'author': '', 'chunk': 3, 'filename': '0aad043d-7bba-42fe-acb1-16b78ffdc59c.pdf', 'page_number': 6, 'source': 'p6-3', 'title': 'None'}, page_content='Vraag 8, 9, 10 en 11 Hoe stelt u lange termijn investeringen in ons verdienvermogen veilig, zoals bijvoorbeeld het Nationaal Groeifonds?'),\n",
       "  0.6709035634994507),\n",
       " (Document(metadata={'Source': 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/Document(0aad043d-7bba-42fe-acb1-16b78ffdc59c)/resource', 'Subject': \"Antwoord op vragen van de leden Tjeerd de Groot en Sneller over het bericht 'Werkgevers: bedrijven verplaatsen nieuwe investeringen naar het buitenland'\", 'author': '', 'chunk': 4, 'filename': '0aad043d-7bba-42fe-acb1-16b78ffdc59c.pdf', 'page_number': 6, 'source': 'p6-4', 'title': 'None'}, page_content='Kunt u in kaart brengen wat de nadelige (economische) effecten zijn van respectievelijk het korten op het Nationaal Groeifonds en het snijden in andere huidige investeringen ter bevordering van verduurzaming en innovatie van het midden- en kleinbedrijf (mkb) en grootbedrijf?'),\n",
       "  0.6202147006988525)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querier.get_documents_with_scores(\"\"\"Vraag 2\n",
    "Kunt u in kaart brengen hoeveel en mogelijk welke bedrijven nieuwe\n",
    "investeringen doen in het buitenland in plaats van Nederland, of dit\n",
    "overwegen? Wat zijn hiervoor de voornaamste drijfveren?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
