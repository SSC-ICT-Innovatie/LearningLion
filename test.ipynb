{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imports\n",
    "import os\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "from loguru import logger\n",
    "# local imports\n",
    "from ingest.ingester import Ingester\n",
    "from query.querier import Querier\n",
    "from summarize.summarizer import Summarizer\n",
    "import settings\n",
    "import utils as ut\n",
    "from query.querier import EnumMode\n",
    "from ingest.ingester import IngestionMode\n",
    "\n",
    "import kamervragenEvaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT= \"\"\"\n",
    "### OBJECTIVE ###\n",
    "Je bent een assistent voor de rijksoverheid. Jouw taak is om vragen te beantwoorden in het Nederlands. Zorg ervoor dat je alleen antwoord geeft op basis van de beschikbare context en dat je daar ook naar verwijst in je antwoord.\n",
    "\n",
    "### AUDIENCE ###\n",
    "De doelgroep van jouw antwoorden zijn ambtenaren. Geef alle relevante informatie uit de context, antwoord in het Nederlands leg in maximaal 100 woorden zoveel mogelijk uit.\n",
    "\n",
    "### GUARDRAILS ###\n",
    "Indien de context onvoldoende informatie bevat om de vraag te beantwoorden, verzin dan geen informatie maar geef aan dat er onvoldoende informatie beschikbaar is.\n",
    "\n",
    "### INSTRUCTIONS ###\n",
    "- Beantwoord de vraag altijd in het Nederlands, zelfs als de context in het Engels is gesteld.\n",
    "- Vermijd het herhalen van de vraag in het antwoord en het herhalen van de instructies. Voer de instructies uit en geef een concreet antwoord op de gestelde vraag.\n",
    "- Geef een stapsgewijze redenering bij het beantwoorden van de vraag en refereer naar specifieke zinnen uit de context die hebben bijgedragen aan het antwoord.\n",
    "- Houd je antwoord nauw verbonden met de context en vermijd het toevoegen van informatie die niet expliciet in de context wordt vermeld.\n",
    "\n",
    "- Voor meer informatie over de context, zeg het bestandsnaam die gevonden is in de source_document. Mits deze beschikbaar is.\n",
    "### QUESTION ### \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Commented the settings that arent used as parameters in the functions\n",
    "\n",
    "# DOC_DIR = \"./docs\"\n",
    "# CHUNK_DIR = \"./chunks\"\n",
    "# VECDB_DIR = \"./vector_stores\"\n",
    "# EVAL_DIR = \"./evaluate\"\n",
    "# EVAL_APP_HEADER = \"Evaluation\"\n",
    "# EVAL_APP_INFO = \"./info/evaluation_explanation.txt\"\n",
    "# EVAL_FILE_NAME = \"eval.json\"\n",
    "# CHAIN_VERBOSITY = False\n",
    "LLM_TYPE = \"local_llm\"\n",
    "LLM_MODEL_TYPE = \"gemma2\"\n",
    "# API_URL = \"http://127.0.0.1:11434\"\n",
    "AZUREOPENAI_API_VERSION = \"2023-08-01-preview\"\n",
    "EMBEDDINGS_PROVIDER = \"local_embeddings\"\n",
    "EMBEDDINGS_MODEL = \"textgain/allnli-GroNLP-bert-base-dutch-cased\"\n",
    "TEXT_SPLITTER_METHOD = \"NLTKTextSplitter\"\n",
    "# CHAIN_NAME = \"conversationalretrievalchain\"\n",
    "# CHAIN_TYPE = \"stuff\"\n",
    "# SEARCH_TYPE = \"similarity\"\n",
    "# SCORE_THRESHOLD = 0.5\n",
    "VECDB_TYPE = \"chromadb\"\n",
    "CHUNK_SIZE = 1024\n",
    "# CHUNK_K = 4\n",
    "CHUNK_OVERLAP = 256\n",
    "# RETRIEVAL_METHOD = \"regular\"\n",
    "\n",
    "\n",
    "folderSelected = \"kamerVragen\"\n",
    "my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected)\n",
    "\n",
    "CONCAT_FILES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTING_METHODS = [IngestionMode.question_answer,IngestionMode.token_small,IngestionMode.token_medium,IngestionMode.token_large]\n",
    "CONTEXT_PRESENT= [True, False]\n",
    "EMBEDDINGS_MODELS = [\"GroNLP/bert-base-dutch-cased\",\"textgain/allnli-GroNLP-bert-base-dutch-cased\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\"dunzhang/stella_en_400M_v5\", \"actualdata/jina-embeddings-v3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "querier = None\n",
    "ingester = None\n",
    "\n",
    "def init(LLM_TYPE=LLM_TYPE, LLM_MODEL_TYPE=LLM_MODEL_TYPE, EMBEDDINGS_MODEL=EMBEDDINGS_MODEL, EMBEDDINGS_PROVIDER=EMBEDDINGS_PROVIDER, AZUREOPENAI_API_VERSION=AZUREOPENAI_API_VERSION, TEXT_SPLITTER_METHOD=TEXT_SPLITTER_METHOD, CHUNK_SIZE=CHUNK_SIZE, CHUNK_OVERLAP=CHUNK_OVERLAP, VECDB_TYPE=VECDB_TYPE, vectordb_folder= my_vectordb_folder_path_selected, content_folder=my_folder_path_selected):\n",
    "  # Init\n",
    "  querier = Querier(\n",
    "    llm_type=LLM_TYPE, \n",
    "    llm_model_type=LLM_MODEL_TYPE, \n",
    "    embeddings_model=EMBEDDINGS_MODEL, \n",
    "    embeddings_provider=EMBEDDINGS_PROVIDER, \n",
    "    azureopenai_api_version=AZUREOPENAI_API_VERSION\n",
    "    )\n",
    "\n",
    "  ingester = Ingester(\n",
    "    collection_name=folderSelected, \n",
    "    content_folder=content_folder, \n",
    "    vectordb_folder=vectordb_folder,\n",
    "    embeddings_model=EMBEDDINGS_MODEL,\n",
    "    text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    vecdb_type=VECDB_TYPE\n",
    "    )\n",
    "  return [querier,ingester]\n",
    "  \n",
    "# querier,ingester = init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(mode=IngestionMode.question_answer_per_page, forceRebuild=True, addedMetaDataURLCSV=\"docs/metadata.csv\", addContext=True):\n",
    "  ingester.ingest(mode=mode, forceRebuild=forceRebuild, addedMetaDataURLCSV=addedMetaDataURLCSV, addContext=addContext)\n",
    "# ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(vectorDBPATH = my_vectordb_folder_path_selected):\n",
    "  querier.make_chain(folderSelected, vectorDBPATH)\n",
    "# chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-10-02 14:46:37.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: textgain/allnli-GroNLP-bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:37.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mExtracting metadata\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mNone\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.file_parser\u001b[0m:\u001b[36mparse_pdf\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mExtracting pages\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done writing to csv\n"
     ]
    }
   ],
   "source": [
    "question_sample_CSV = \"question_sample.csv\"\n",
    "\n",
    "querier,ingester = init(EMBEDDINGS_MODEL=EMBEDDINGS_MODEL, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "kamervragenEvaluation.create_evaluation_sample_questions(my_folder_path_selected,ingester=ingester, destinationCSV=question_sample_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place folder contents in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name GroNLP/bert-base-dutch-cased. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-10-02 14:46:38.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: GroNLP/bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:38.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "No sentence-transformers model found with name GroNLP/bert-base-dutch-cased. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-10-02 14:46:39.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: GroNLP/bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:39.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mSkipping ingestion of file .DS_Store because it has extension tore\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:39.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mVector store to be created for folder ./docs/kamerVragen\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect to tenant default_tenant. Are you sure it exists?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-10-02 14:46:41.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: textgain/allnli-GroNLP-bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:41.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:41.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:41.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:41.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-10-02 14:46:43.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: textgain/allnli-GroNLP-bert-base-dutch-cased\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:43.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mSkipping ingestion of file .DS_Store because it has extension tore\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:43.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mVector store to be created for folder ./docs/kamerVragen\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect to tenant default_tenant. Are you sure it exists?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-02 14:46:45.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:45.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:45.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:45.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:45.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-10-02 14:46:48.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:48.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mSkipping ingestion of file .DS_Store because it has extension tore\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:48.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mVector store to be created for folder ./docs/kamerVragen\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect to tenant default_tenant. Are you sure it exists?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m2024-10-02 14:46:52.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: dunzhang/stella_en_400M_v5\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:52.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mUse Local LLM\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:52.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mRetrieving gemma2\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:52.472\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mUsing local api url http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:52.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mllm_class.llm_class\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mRetrieved gemma2\u001b[0m\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m2024-10-02 14:46:57.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils\u001b[0m:\u001b[36mgetEmbeddings\u001b[0m:\u001b[36m118\u001b[0m - \u001b[1mLoaded local embeddings: dunzhang/stella_en_400M_v5\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:57.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mSkipping ingestion of file .DS_Store because it has extension tore\u001b[0m\n",
      "\u001b[32m2024-10-02 14:46:57.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mingest.ingester\u001b[0m:\u001b[36mingest\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mVector store to be created for folder ./docs/kamerVragen\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not connect to tenant default_tenant. Are you sure it exists?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  14%|█▍        | 1/7 [17:56<1:47:41, 1076.92s/it]Error while downloading from https://cdn-lfs-us-1.hf.co/repos/23/5a/235a79d15bd2d60f030ad9b3ca026d35127fb12f2f44ce99925914e1eebcc476/d6c947d8a589b9e8ce836166a979d2d245eec6a8c611a31869e559bb4e8ed811?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00007.safetensors%3B+filename%3D%22model-00002-of-00007.safetensors%22%3B&Expires=1728133496&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyODEzMzQ5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzIzLzVhLzIzNWE3OWQxNWJkMmQ2MGYwMzBhZDliM2NhMDI2ZDM1MTI3ZmIxMmYyZjQ0Y2U5OTkyNTkxNGUxZWViY2M0NzYvZDZjOTQ3ZDhhNTg5YjllOGNlODM2MTY2YTk3OWQyZDI0NWVlYzZhOGM2MTFhMzE4NjllNTU5YmI0ZThlZDgxMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=q91g8hpQMIuAugQYKLzwXFIiKchTJLMFgIvZzzMtLK5usJ-da0FpjZwfWW19PHmvRVQgegiU7Ae9sZwr7uzQN5hpHUiINMq3UXABD-BTVBjwiBZ52V-oVjifRQnuDjWZt6A0uAlDZvalrWmORqunEqr-10GvVujs2kJuPtiJfBrZglJFfHXUnVy5mNO60%7EFyPJ-4fX151hiLBi4z3jZsN8T9xKjMCrbE43Jkfpm%7Ej1Hnrmhf7NT5UvhbFqKkAj7FIDQ3lFhTPGEtcFbEHqKEV9cJotikdy7QEFNraqdcwWH0Bd1fCDcePTqibziC1XUDYIJeU%7E-jxjrJl07VAgDvyQ__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Downloading shards:  29%|██▊       | 2/7 [36:33<1:31:23, 1096.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Setup\u001b[39;00m\n\u001b[1;32m     19\u001b[0m my_folder_path_selected, my_vectordb_folder_path_selected \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39mcreate_vectordb_name(folderSelected, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, splitting_method\u001b[38;5;241m=\u001b[39msplittingMethod, embeddings_model\u001b[38;5;241m=\u001b[39membeddingModel, added_context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[0;32m---> 20\u001b[0m querier,ingester \u001b[38;5;241m=\u001b[39m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDINGS_MODEL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddingModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectordb_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_vectordb_folder_path_selected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Ingestion\u001b[39;00m\n\u001b[1;32m     22\u001b[0m ingest(mode\u001b[38;5;241m=\u001b[39msplittingMethod, addContext\u001b[38;5;241m=\u001b[39mcontext, addedMetaDataURLCSV\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs/metadata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36minit\u001b[0;34m(LLM_TYPE, LLM_MODEL_TYPE, EMBEDDINGS_MODEL, EMBEDDINGS_PROVIDER, AZUREOPENAI_API_VERSION, TEXT_SPLITTER_METHOD, CHUNK_SIZE, CHUNK_OVERLAP, VECDB_TYPE, vectordb_folder, content_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit\u001b[39m(LLM_TYPE\u001b[38;5;241m=\u001b[39mLLM_TYPE, LLM_MODEL_TYPE\u001b[38;5;241m=\u001b[39mLLM_MODEL_TYPE, EMBEDDINGS_MODEL\u001b[38;5;241m=\u001b[39mEMBEDDINGS_MODEL, EMBEDDINGS_PROVIDER\u001b[38;5;241m=\u001b[39mEMBEDDINGS_PROVIDER, AZUREOPENAI_API_VERSION\u001b[38;5;241m=\u001b[39mAZUREOPENAI_API_VERSION, TEXT_SPLITTER_METHOD\u001b[38;5;241m=\u001b[39mTEXT_SPLITTER_METHOD, CHUNK_SIZE\u001b[38;5;241m=\u001b[39mCHUNK_SIZE, CHUNK_OVERLAP\u001b[38;5;241m=\u001b[39mCHUNK_OVERLAP, VECDB_TYPE\u001b[38;5;241m=\u001b[39mVECDB_TYPE, vectordb_folder\u001b[38;5;241m=\u001b[39m my_vectordb_folder_path_selected, content_folder\u001b[38;5;241m=\u001b[39mmy_folder_path_selected):\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;66;03m# Init\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m   querier \u001b[38;5;241m=\u001b[39m \u001b[43mQuerier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLLM_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_model_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLLM_MODEL_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDINGS_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDINGS_PROVIDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mazureopenai_api_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAZUREOPENAI_API_VERSION\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m   ingester \u001b[38;5;241m=\u001b[39m Ingester(\n\u001b[1;32m     15\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mfolderSelected, \n\u001b[1;32m     16\u001b[0m     content_folder\u001b[38;5;241m=\u001b[39mcontent_folder, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     vecdb_type\u001b[38;5;241m=\u001b[39mVECDB_TYPE\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [querier,ingester]\n",
      "File \u001b[0;32m~/Documents/GitHub/LearningLion-kamervragen/query/querier.py:54\u001b[0m, in \u001b[0;36mQuerier.__init__\u001b[0;34m(self, llm_type, llm_model_type, embeddings_provider, embeddings_model, vecdb_type, chain_name, chain_type, chain_verbosity, search_type, score_threshold, chunk_k, local_api_url, azureopenai_api_version)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mazureopenai_api_version \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mAZUREOPENAI_API_VERSION \\\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m azureopenai_api_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mAZUREOPENAI_API_VERSION \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m azureopenai_api_version\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_api_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mazureopenai_api_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# define llm\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m LLM(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_api_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mazureopenai_api_version)\u001b[38;5;241m.\u001b[39mget_llm()\n",
      "File \u001b[0;32m~/Documents/GitHub/LearningLion-kamervragen/utils.py:113\u001b[0m, in \u001b[0;36mgetEmbeddings\u001b[0;34m(embeddings_provider, embeddings_model, local_api_url, azureopenai_api_version)\u001b[0m\n\u001b[1;32m    111\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     encode_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[0;32m--> 113\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded local embeddings: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m embeddings_model)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m embeddings_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:213\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     emit_warning()\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_community/embeddings/huggingface.py:92\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43msentence_transformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:294\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    285\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    288\u001b[0m     model_name_or_path,\n\u001b[1;32m    289\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    293\u001b[0m ):\n\u001b[0;32m--> 294\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    307\u001b[0m         model_name_or_path,\n\u001b[1;32m    308\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    316\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1647\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1647\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1649\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(model_name_or_path)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:56\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     55\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     59\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:87\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:3769\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3768\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3769\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3781\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3785\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3786\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3787\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3788\u001b[0m ):\n\u001b[1;32m   3789\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1381\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    543\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "VALIDATIONLAPS = 10\n",
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for time in range(VALIDATIONLAPS):\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "      for embeddingModel in EMBEDDINGS_MODELS:\n",
    "        current_item += 1\n",
    "        try:\n",
    "          chunk_size = CHUNK_SIZE\n",
    "          if splittingMethod == IngestionMode.token_small:\n",
    "            chunk_size = 128\n",
    "          elif splittingMethod == IngestionMode.token_medium:\n",
    "            chunk_size = 512\n",
    "          elif splittingMethod == IngestionMode.token_large:\n",
    "            chunk_size = 1024\n",
    "            \n",
    "          # Setup\n",
    "          my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "          querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "          # Ingestion\n",
    "          ingest(mode=splittingMethod, addContext=context, addedMetaDataURLCSV=\"docs/metadata.csv\")\n",
    "          # RAG CHAIN\n",
    "          chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "          \n",
    "          # Evaluation\n",
    "          kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "            question_sample_CSV,querier=querier, \n",
    "            toCSV=True, \n",
    "            ingestionMode=splittingMethod, \n",
    "            addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "            addContext=context,\n",
    "            embeddings_model=embeddingModel,\n",
    "            text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "            embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "            database=VECDB_TYPE,\n",
    "            concatFiles=CONCAT_FILES,\n",
    "            )\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          # Write error to file\n",
    "          with open(\"error.txt\", \"a\") as f:\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            f.write(f\"Error [{time}]: {e} \\n\")\n",
    "          continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"done with {current_item} of {total_items}\")\n",
    "  print(f\"Done with iteration {time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "LongTimeEmbeddingsModels = [\"BAAI/bge-multilingual-gemma2\", \"Alibaba-NLP/gte-Qwen2-7B-instruct\",\"Alibaba-NLP/gte-multilingual-base\"]\n",
    "\n",
    "VALIDATIONLAPS = 10\n",
    "current_item = 0\n",
    "total_items = VALIDATIONLAPS * len(SPLITTING_METHODS) * len(CONTEXT_PRESENT) * len(EMBEDDINGS_MODELS)\n",
    "for time in range(VALIDATIONLAPS):\n",
    "  for splittingMethod in SPLITTING_METHODS:\n",
    "    for context in CONTEXT_PRESENT:\n",
    "      for embeddingModel in LongTimeEmbeddingsModels:\n",
    "        current_item += 1\n",
    "        try:\n",
    "          chunk_size = CHUNK_SIZE\n",
    "          if splittingMethod == IngestionMode.token_small:\n",
    "            chunk_size = 128\n",
    "          elif splittingMethod == IngestionMode.token_medium:\n",
    "            chunk_size = 512\n",
    "          elif splittingMethod == IngestionMode.token_large:\n",
    "            chunk_size = 1024\n",
    "            \n",
    "          # Setup\n",
    "          my_folder_path_selected, my_vectordb_folder_path_selected = ut.create_vectordb_name(folderSelected, chunk_size=chunk_size, chunk_overlap=0, splitting_method=splittingMethod, embeddings_model=embeddingModel, added_context=context)\n",
    "          querier,ingester = init(EMBEDDINGS_MODEL=embeddingModel, vectordb_folder=my_vectordb_folder_path_selected)\n",
    "          # Ingestion\n",
    "          ingest(mode=splittingMethod, addContext=context, addedMetaDataURLCSV=\"docs/metadata.csv\")\n",
    "          # RAG CHAIN\n",
    "          chain(vectorDBPATH=my_vectordb_folder_path_selected)\n",
    "          \n",
    "          # Evaluation\n",
    "          kamervragenEvaluation.evaluate_with_sample_questions(\n",
    "            question_sample_CSV,querier=querier, \n",
    "            toCSV=True, \n",
    "            ingestionMode=splittingMethod, \n",
    "            addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "            addContext=context,\n",
    "            embeddings_model=embeddingModel,\n",
    "            text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "            embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "            database=VECDB_TYPE,\n",
    "            concatFiles=CONCAT_FILES,\n",
    "            )\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "          # Write error to file\n",
    "          with open(\"error.txt\", \"a\") as f:\n",
    "            time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            f.write(f\"Error [{time}]: {e} \\n\")\n",
    "          continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"done with {current_item} of {total_items}\")\n",
    "  print(f\"Done with iteration {time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kamervragenEvaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if one single file can be retrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kamervragenEvaluation.test_retrival(\n",
    "  my_folder_path_selected, \n",
    "  ingester, \n",
    "  querier=querier, \n",
    "  toCSV=True,\n",
    "  ingestionMode=IngestionMode.question_answer_per_page, \n",
    "  addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "  addContext=True,\n",
    "  embeddings_model=EMBEDDINGS_MODEL,\n",
    "  text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "  embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "  database=VECDB_TYPE,\n",
    "  ConcatFiles=CONCAT_FILES\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamervragenEvaluation.store_questions_and_answers_CSV(my_folder_path_selected, ingester,concatFiles=CONCAT_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kamervragenEvaluation.test_retrival_map_grading(\n",
    "  my_folder_path_selected, \n",
    "  ingester, \n",
    "  querier=querier, \n",
    "  toCSV=True,\n",
    "  ingestionMode=IngestionMode.question_answer_per_page, \n",
    "  addedMetaDataURLCSV=\"docs/metadata.csv\", \n",
    "  addContext=True,\n",
    "  embeddings_model=EMBEDDINGS_MODEL,\n",
    "  text_splitter_method=TEXT_SPLITTER_METHOD,\n",
    "  embeddings_provider=EMBEDDINGS_PROVIDER,\n",
    "  database=VECDB_TYPE,\n",
    "  concatFiles=CONCAT_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
