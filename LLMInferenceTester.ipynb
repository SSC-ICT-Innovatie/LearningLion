{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Tester\n",
    "\n",
    "What is the best method the use LLM inference? \n",
    "VLLM has been omitted due to permission errors. \n",
    "TENSORRTLLM has been omitted due to failure to install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def write_to_CSV(model, starttime, endtime, type, method, response=None):\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Model\": [model],  # Wrapping scalar values in lists\n",
    "        \"starttime\": [starttime],\n",
    "        \"endtime\": [endtime],\n",
    "        \"duration\": [endtime - starttime],\n",
    "        \"type\": [type],\n",
    "        \"method\": [method],\n",
    "        \"Response\": [response]\n",
    "    })\n",
    "    \n",
    "    file_path = \"modelTimings.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # If the file exists, read it and concatenate the new data\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        # If the file doesn't exist, the new data becomes the updated data\n",
    "        updated_data = new_data\n",
    "        \n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "def clean():\n",
    "    torch.cuda.empty_cache()\n",
    "modelNames = [\"BramVanroy/fietje-2-chat\",\"BramVanroy/fietje-2\",\"BramVanroy/GEITje-7B-ultra\",\"Rijgersberg/GEITje-7B\",\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
    "prompts = [\"Wat is de hoofdstad van Nederland?\", \"Wie is de primeur van nederland?\", \"Wat is het kwadraat van 5\", \"Hoeveel p's zijn er in appel?\"]\n",
    "systemPrompt = \"Je bent een vriendelijk chatbot die graag vragen beantwoordt en altijd zijn best doet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "loadedModelPipeline = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"transformers-pipeline\"\n",
    "    startTime = time.time()\n",
    "\n",
    "    generator = pipeline(model=modelName)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for generator in loadedModelPipeline:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    response = generator(translatedPrompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "action = \"Load\"\n",
    "method = \"AutoModelForCausalLM\"\n",
    "loadedModelCasual = []\n",
    "for modelName in modelNames:\n",
    "    startTime = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, trust_remote_code=True, device_map='auto')\n",
    "    loadedModelCasual.append(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for model in loadedModelCasual:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method,response)\n",
    "  del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "There is no automatic download for ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# startTime = time.time()\n",
    "# response = ollama.chat(model='llama3.1', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# write_to_CSV(\"llama3.1\", startTime, time.time(), \"Inference\", \"ollama\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/optimum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "loadedModelOptimum = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"optimum.onnxruntime\"\n",
    "    startTime = time.time()\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(modelName,from_transformers=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    generator = pipeline(model=model,tokenizer=tokenizer)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in loadedModelOptimum:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    response = generator(prompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_skip_modules=[\"lm_head\"])\n",
    "models8Bit = []\n",
    "method = \"8bit quantization with BitsAndBytesConfig\"\n",
    "for modelName in modelNames:\n",
    "    timeStart = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=quantization_config)\n",
    "    models8Bit.append(model)\n",
    "    write_to_CSV(modelName, timeStart, time.time(), \"Load\", method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for model in models8Bit:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method,response)\n",
    "  del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
