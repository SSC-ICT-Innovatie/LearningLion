{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Tester\n",
    "\n",
    "What is the best method the use LLM inference? \n",
    "VLLM has been omitted due to permission errors. \n",
    "TENSORRTLLM has been omitted due to failure to install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def write_to_CSV(model, starttime, endtime, type, method, response=None):\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Model\": [model],  # Wrapping scalar values in lists\n",
    "        \"starttime\": [starttime],\n",
    "        \"endtime\": [endtime],\n",
    "        \"duration\": [endtime - starttime],\n",
    "        \"type\": [type],\n",
    "        \"method\": [method],\n",
    "        \"Response\": [response]\n",
    "    })\n",
    "    \n",
    "    file_path = \"modelTimings.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # If the file exists, read it and concatenate the new data\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        # If the file doesn't exist, the new data becomes the updated data\n",
    "        updated_data = new_data\n",
    "        \n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "def clean():\n",
    "    torch.cuda.empty_cache()\n",
    "modelNames = [\"BramVanroy/fietje-2-chat\",\"BramVanroy/fietje-2\",\"BramVanroy/GEITje-7B-ultra\",\"Rijgersberg/GEITje-7B\",\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
    "prompts = [\"Wat is de hoofdstad van Nederland?\", \"Wie is de primeur van nederland?\", \"Wat is het kwadraat van 5\", \"Hoeveel p's zijn er in appel?\"]\n",
    "systemPrompt = \"Je bent een vriendelijk chatbot die graag vragen beantwoordt en altijd zijn best doet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "loadedModelPipeline = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"transformers-pipeline\"\n",
    "    startTime = time.time()\n",
    "\n",
    "    generator = pipeline(model=modelName)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for generator in loadedModelPipeline:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    response = generator(translatedPrompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "action = \"Load\"\n",
    "method = \"AutoModelForCausalLM\"\n",
    "loadedModelCasual = []\n",
    "for modelName in modelNames:\n",
    "    startTime = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, trust_remote_code=True, device_map='auto')\n",
    "    loadedModelCasual.append(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for model in loadedModelCasual:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method,response)\n",
    "  del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "There is no automatic download for ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# startTime = time.time()\n",
    "# response = ollama.chat(model='llama3.1', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# write_to_CSV(\"llama3.1\", startTime, time.time(), \"Inference\", \"ollama\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/optimum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "loadedModelOptimum = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"optimum.onnxruntime\"\n",
    "    startTime = time.time()\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(modelName,from_transformers=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    generator = pipeline(model=model,tokenizer=tokenizer)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in loadedModelOptimum:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    response = generator(prompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modelName \u001b[38;5;129;01min\u001b[39;00m modelNames:\n\u001b[1;32m      6\u001b[0m     timeStart \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     models8Bit\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m      9\u001b[0m     write_to_CSV(modelName, timeStart, time\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8bit quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_skip_modules=[\"lm_head\"])\n",
    "models8Bit = []\n",
    "method = \"8bit quantization with BitsAndBytesConfig\"\n",
    "for modelName in modelNames:\n",
    "    timeStart = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=quantization_config)\n",
    "    models8Bit.append(model)\n",
    "    write_to_CSV(modelName, timeStart, time.time(), \"Load\", method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for model in models8Bit:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method,response)\n",
    "  del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclean\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean' is not defined"
     ]
    }
   ],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
