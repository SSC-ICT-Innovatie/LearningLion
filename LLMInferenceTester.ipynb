{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Tester\n",
    "\n",
    "What is the best method the use LLM inference? \n",
    "VLLM has been omitted due to permission errors. \n",
    "TENSORRTLLM has been omitted due to failure to install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def write_to_CSV(model, starttime, endtime, type, method, response=None):\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Model\": [model],  # Wrapping scalar values in lists\n",
    "        \"starttime\": [starttime],\n",
    "        \"endtime\": [endtime],\n",
    "        \"duration\": [endtime - starttime],\n",
    "        \"type\": [type],\n",
    "        \"method\": [method],\n",
    "        \"Response\": [response]\n",
    "    })\n",
    "    \n",
    "    file_path = \"modelTimings.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # If the file exists, read it and concatenate the new data\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        # If the file doesn't exist, the new data becomes the updated data\n",
    "        updated_data = new_data\n",
    "        \n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "def clean():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# modelNames = [\"BramVanroy/fietje-2-chat\",\"BramVanroy/fietje-2\",\"BramVanroy/GEITje-7B-ultra\",\"Rijgersberg/GEITje-7B\",\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
    "modelNames = [\"BramVanroy/fietje-2-chat\"]\n",
    "prompts = [\"Wat is de hoofdstad van Nederland?\", \"Wie is de primeur van nederland?\", \"Wat is het kwadraat van 5\", \"Hoeveel p's zijn er in appel?\"]\n",
    "systemPrompt = \"Je bent een vriendelijk chatbot die graag vragen beantwoordt en altijd zijn best doet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "loadedModelPipeline = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"transformers-pipeline\"\n",
    "    startTime = time.time()\n",
    "\n",
    "    generator = pipeline(model=modelName, device=\"cuda\")\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for generator in loadedModelPipeline:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    response = generator(translatedPrompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator\n",
    "loadedModelPipeline = []\n",
    "del loadedModelPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "action = \"Load\"\n",
    "method = \"AutoModelForCausalLM\"\n",
    "loadedModelCasual = []\n",
    "loadedTokenizers = []\n",
    "for modelName in modelNames:\n",
    "    startTime = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, trust_remote_code=True, device_map='auto')\n",
    "    loadedModelCasual.append(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    loadedTokenizers.append(tokenizer)\n",
    "    # input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)\n",
    "    print(model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for idx, model in enumerate(loadedModelCasual):\n",
    "    tokenizer = loadedTokenizers[idx]\n",
    "    for prompt in prompts:\n",
    "        translatedPrompt = tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": systemPrompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(translatedPrompt, return_tensors='pt', padding=True).to(model.device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        startTime = time.time()\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        write_to_CSV(modelNames[idx], startTime, time.time(), action, method, response)\n",
    "        print(response)\n",
    "        \n",
    "    del model\n",
    "    del tokenizer\n",
    "    \n",
    "loadedTokenizers = []\n",
    "del loadedTokenizers\n",
    "loadedModelCasual = []\n",
    "del loadedModelCasual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "There is no automatic download for ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# startTime = time.time()\n",
    "# response = ollama.chat(model='llama3.1', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# write_to_CSV(\"llama3.1\", startTime, time.time(), \"Inference\", \"ollama\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/optimum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "loadedModelOptimum = []\n",
    "loadedTokenizersOptimum = []\n",
    "loadedModelPipeline = []\n",
    "\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"optimum.onnxruntime\"\n",
    "    startTime = time.time()\n",
    "    model = ORTModelForCausalLM.from_pretrained(modelName, export=True, providers=[\"CUDAExecutionProvider\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    loadedTokenizersOptimum.append(tokenizer)\n",
    "    loadedModelOptimum.append(model)\n",
    "    generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)\n",
    "    print(model.config._name_or_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(loadedModelOptimum):\n",
    "    tokenizer = loadedTokenizersOptimum[idx]\n",
    "    \n",
    "    # Ensure the model is on CUDA\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        translatedPrompt = tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": systemPrompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(translatedPrompt, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        startTime = time.time()\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=1024, num_return_sequences=1)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        write_to_CSV(model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "        print(response)\n",
    "    \n",
    "    # Optionally delete the generator\n",
    "    del model\n",
    "    del tokenizer\n",
    "loadedTokenizersOptimum = []\n",
    "del loadedTokenizersOptimum\n",
    "loadedModelOptimum = []\n",
    "del loadedModelOptimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_skip_modules=[\"lm_head\"])\n",
    "models8Bit = []\n",
    "tokenizer8Bit = []\n",
    "method = \"8bit quantization with BitsAndBytesConfig\"\n",
    "for modelName in modelNames:\n",
    "    timeStart = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    tokenizer8Bit.append(tokenizer)\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=quantization_config)\n",
    "    models8Bit.append(model)\n",
    "    write_to_CSV(modelName, timeStart, time.time(), \"Load\", method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for idx, model in enumerate(models8Bit):\n",
    "    tokenizer = tokenizer8Bit[idx]\n",
    "    for prompt in prompts:\n",
    "        # Translate the system and user prompt\n",
    "        translatedPrompt = tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": systemPrompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenize the translated prompt\n",
    "        input_ids = tokenizer(translatedPrompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # Generate response (timing starts here)\n",
    "        startTime = time.time()\n",
    "        outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "        endTime = time.time()\n",
    "\n",
    "        # Decode the generated output\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(response)\n",
    "        # Write the result to CSV\n",
    "        write_to_CSV(modelNames[idx], startTime, endTime, action, method, response)\n",
    "\n",
    "    # Clean up the model to free memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "tokenizer8Bit = []\n",
    "del tokenizer8Bit\n",
    "models8Bit = []\n",
    "del models8Bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
