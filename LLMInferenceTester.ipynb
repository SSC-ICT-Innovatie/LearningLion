{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Tester\n",
    "\n",
    "What is the best method the use LLM inference? \n",
    "VLLM has been omitted due to permission errors. \n",
    "TENSORRTLLM has been omitted due to failure to install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def write_to_CSV(model, starttime, endtime, type, method, response=None):\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Model\": [model],  # Wrapping scalar values in lists\n",
    "        \"starttime\": [starttime],\n",
    "        \"endtime\": [endtime],\n",
    "        \"duration\": [endtime - starttime],\n",
    "        \"type\": [type],\n",
    "        \"method\": [method],\n",
    "        \"Response\": [response]\n",
    "    })\n",
    "    \n",
    "    file_path = \"modelTimings.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # If the file exists, read it and concatenate the new data\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        # If the file doesn't exist, the new data becomes the updated data\n",
    "        updated_data = new_data\n",
    "        \n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "def clean():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# modelNames = [\"BramVanroy/fietje-2-chat\",\"BramVanroy/fietje-2\",\"BramVanroy/GEITje-7B-ultra\",\"Rijgersberg/GEITje-7B\",\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
    "modelNames = [\"BramVanroy/fietje-2-chat\"]\n",
    "prompts = [\"Wat is de hoofdstad van Nederland?\", \"Wie is de primeur van nederland?\", \"Wat is het kwadraat van 5\", \"Hoeveel p's zijn er in appel?\"]\n",
    "systemPrompt = \"Je bent een vriendelijk chatbot die graag vragen beantwoordt en altijd zijn best doet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "loadedModelPipeline = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"transformers-pipeline\"\n",
    "    startTime = time.time()\n",
    "\n",
    "    generator = pipeline(model=modelName)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for generator in loadedModelPipeline:\n",
    "  for prompt in prompts:\n",
    "    #  This is not being included in the timing\n",
    "    translatedPrompt = generator.tokenizer.apply_chat_template([\n",
    "      {\"role\": \"system\", \"content\": systemPrompt},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "      ], tokenize=False, add_generation_prompt=True)\n",
    "    startTime = time.time()\n",
    "    response = generator(translatedPrompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator\n",
    "loadedModelPipeline = []\n",
    "del loadedModelPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "action = \"Load\"\n",
    "method = \"AutoModelForCausalLM\"\n",
    "loadedModelCasual = []\n",
    "loadedTokenizers = []\n",
    "for modelName in modelNames:\n",
    "    startTime = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, trust_remote_code=True, device_map='auto')\n",
    "    loadedModelCasual.append(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    loadedTokenizers.append(tokenizer)\n",
    "    # input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)\n",
    "    print(model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for idx, model in enumerate(loadedModelCasual):\n",
    "    tokenizer = loadedTokenizers[idx]\n",
    "    for prompt in prompts:\n",
    "        translatedPrompt = tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": systemPrompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(translatedPrompt, return_tensors='pt', padding=True).to(model.device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        startTime = time.time()\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        write_to_CSV(modelNames[idx], startTime, time.time(), action, method, response)\n",
    "        print(response)\n",
    "        \n",
    "    del model\n",
    "    del tokenizer\n",
    "    \n",
    "loadedTokenizers = []\n",
    "del loadedTokenizers\n",
    "loadedModelCasual = []\n",
    "del loadedModelCasual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "There is no automatic download for ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# startTime = time.time()\n",
    "# response = ollama.chat(model='llama3.1', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# write_to_CSV(\"llama3.1\", startTime, time.time(), \"Inference\", \"ollama\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "loadedModelOptimum = []\n",
    "loadedTokenizersOptimum = []\n",
    "loadedModelPipeline = []\n",
    "\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"optimum.onnxruntime\"\n",
    "    startTime = time.time()\n",
    "    model = ORTModelForCausalLM.from_pretrained(modelName, export=True, provider=\"CUDAExecutionProvider\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    loadedTokenizersOptimum.append(tokenizer)\n",
    "    loadedModelOptimum.append(model)\n",
    "    generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)\n",
    "    print(model.config._name_or_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(loadedModelOptimum):\n",
    "    tokenizer = loadedTokenizersOptimum[idx]\n",
    "    \n",
    "    # Ensure the model is on CUDA\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        translatedPrompt = tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": systemPrompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(translatedPrompt, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        startTime = time.time()\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=1024, num_return_sequences=1)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        write_to_CSV(model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "        print(response)\n",
    "    \n",
    "    # Optionally delete the generator\n",
    "    del model\n",
    "    del tokenizer\n",
    "loadedTokenizersOptimum = []\n",
    "del loadedTokenizersOptimum\n",
    "loadedModelOptimum = []\n",
    "del loadedModelOptimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_skip_modules=[\"lm_head\"])\n",
    "models8Bit = []\n",
    "tokenizer8Bit = []\n",
    "method = \"8bit quantization with BitsAndBytesConfig\"\n",
    "for modelName in modelNames:\n",
    "    timeStart = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    tokenizer8Bit.append(tokenizer)\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=quantization_config)\n",
    "    models8Bit.append(model)\n",
    "    write_to_CSV(modelName, timeStart, time.time(), \"Load\", method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for idx, model in enumerate(models8Bit):\n",
    "    tokenizer = tokenizer8Bit[idx]\n",
    "    for prompt in prompts:\n",
    "        # Translate the system and user prompt\n",
    "        translatedPrompt = tokenizer.apply_chat_template([\n",
    "            {\"role\": \"system\", \"content\": systemPrompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenize the translated prompt\n",
    "        input_ids = tokenizer(translatedPrompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # Generate response (timing starts here)\n",
    "        startTime = time.time()\n",
    "        outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "        endTime = time.time()\n",
    "\n",
    "        # Decode the generated output\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(response)\n",
    "        # Write the result to CSV\n",
    "        write_to_CSV(modelNames[idx], startTime, endTime, action, method, response)\n",
    "\n",
    "    # Clean up the model to free memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "tokenizer8Bit = []\n",
    "del tokenizer8Bit\n",
    "models8Bit = []\n",
    "del models8Bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self quantization (BEAWARE THIS TAKES A LONG TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for auto-gptq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m      7\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m GPTQConfig(bits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc4\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:3452\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3449\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3452\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3456\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py:52\u001b[0m, in \u001b[0;36mGptqHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 52\u001b[0m     gptq_supports_cpu \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto-gptq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.4.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gptq_supports_cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU is required to quantize or run quantize model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:1009\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:982\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    977\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py:565\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for auto-gptq"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "\n",
    "model_id = modelNames[0]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    translatedPrompt = tokenizer.apply_chat_template([\n",
    "        {\"role\": \"system\", \"content\": systemPrompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    model.to(\"cuda\")\n",
    "    inputs = tokenizer(translatedPrompt, return_tensors='pt', padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n",
    "    startTime = time.time()\n",
    "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    write_to_CSV(modelNames[0], startTime, time.time(), \"Inference\", \"self-quanitzed-GPTQ\", response)\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
