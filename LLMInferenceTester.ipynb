{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference Tester\n",
    "\n",
    "What is the best method the use LLM inference? \n",
    "VLLM has been omitted due to permission errors. \n",
    "TENSORRTLLM has been omitted due to failure to install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def write_to_CSV(model, starttime, endtime, type, method, response=None):\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Model\": [model],  # Wrapping scalar values in lists\n",
    "        \"starttime\": [starttime],\n",
    "        \"endtime\": [endtime],\n",
    "        \"duration\": [endtime - starttime],\n",
    "        \"type\": [type],\n",
    "        \"method\": [method],\n",
    "        \"Response\": [response]\n",
    "    })\n",
    "    \n",
    "    file_path = \"modelTimings.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # If the file exists, read it and concatenate the new data\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "        updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        # If the file doesn't exist, the new data becomes the updated data\n",
    "        updated_data = new_data\n",
    "        \n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "modelNames = [\"BramVanroy/fietje-2-chat\",\"BramVanroy/fietje-2\",\"BramVanroy/GEITje-7B-ultra\",\"Rijgersberg/GEITje-7B\",\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
    "prompts = [\"Wat is de hoofdstad van Nederland?\", \"Wie is de primeur van nederland?\", \"Wat is het kwadraat van 5\", \"Hoeveel p's zijn er in appel?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "loadedModelPipeline = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"transformers-pipeline\"\n",
    "    startTime = time.time()\n",
    "\n",
    "    generator = pipeline(model=modelName)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for generator in loadedModelPipeline:\n",
    "  for prompt in prompts:\n",
    "    startTime = time.time()\n",
    "    response = generator(prompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "action = \"Load\"\n",
    "method = \"AutoModelForCausalLM\"\n",
    "loadedModelCasual = []\n",
    "for modelName in modelNames:\n",
    "    startTime = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelName, trust_remote_code=True, device_map='auto')\n",
    "    loadedModelCasual.append(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = \"Inference\"\n",
    "for model in loadedModelCasual:\n",
    "  for prompt in prompts:\n",
    "    startTime = time.time()\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method,response)\n",
    "  del model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "There is no automatic download for ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "# startTime = time.time()\n",
    "# response = ollama.chat(model='llama3.1', messages=[\n",
    "#   {\n",
    "#     'role': 'user',\n",
    "#     'content': 'Why is the sky blue?',\n",
    "#   },\n",
    "# ])\n",
    "# write_to_CSV(\"llama3.1\", startTime, time.time(), \"Inference\", \"ollama\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/optimum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "loadedModelOptimum = []\n",
    "for modelName in modelNames:\n",
    "    # modelName = \"BramVanroy/fietje-2-chat\"\n",
    "    action = \"Load\"\n",
    "    method = \"optimum.onnxruntime\"\n",
    "    startTime = time.time()\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(modelName,from_transformers=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    generator = pipeline(model=model,tokenizer=tokenizer)\n",
    "    print(generator.model.config._name_or_path)\n",
    "    loadedModelPipeline.append(generator)\n",
    "    # Time.Time works in seconds\n",
    "    write_to_CSV(modelName, startTime, time.time(), action, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in loadedModelOptimum:\n",
    "  for prompt in prompts:\n",
    "    startTime = time.time()\n",
    "    response = generator(prompt, max_length=1024, num_return_sequences=1)\n",
    "    write_to_CSV(generator.model.config._name_or_path, startTime, time.time(), action, method, response)\n",
    "    print(response)\n",
    "  del generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
